"use strict";const app=require("../shared/langchainjs.d3941866.cjs");require("path"),require("tty"),require("util"),require("fs"),require("net"),require("events"),require("stream"),require("zlib"),require("buffer"),require("string_decoder"),require("querystring"),require("url"),require("http"),require("crypto"),require("os"),require("https"),require("assert");class BaseChatMessage{constructor(e){Object.defineProperty(this,"text",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.text=e}}class HumanChatMessage extends BaseChatMessage{_getType(){return"human"}}class AIChatMessage extends BaseChatMessage{_getType(){return"ai"}}class SystemChatMessage extends BaseChatMessage{_getType(){return"system"}}class ChatMessage extends BaseChatMessage{constructor(e,r){super(e),Object.defineProperty(this,"role",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.role=r}_getType(){return"generic"}}function getBufferString(s,e="Human",r="AI"){const t=[];for(const n of s){let a;if(n instanceof HumanChatMessage)a=e;else if(n instanceof AIChatMessage)a=r;else if(n instanceof SystemChatMessage)a="System";else if(n instanceof ChatMessage)a=n.role;else throw new Error(`Got unsupported message type: ${n}`);t.push(`${a}: ${n.text}`)}return t.join(`
`)}class BaseChatModel extends app.BaseLanguageModel{constructor({...e}){super(e),Object.defineProperty(this,"_tokenizer",{enumerable:!0,configurable:!0,writable:!0,value:void 0})}async generate(e,r){const t=[],n=[],a=e.map(i=>getBufferString(i));await this.callbackManager.handleLLMStart({name:this._llmType()},a,this.verbose);try{for(const i of e){const l=await this._generate(i,r);l.llmOutput&&n.push(l.llmOutput),t.push(l.generations)}}catch(i){throw await this.callbackManager.handleLLMError(i,this.verbose),i}const c={generations:t,llmOutput:n.length?this._combineLLMOutput?.(...n):void 0};return await this.callbackManager.handleLLMEnd(c,this.verbose),c}_modelType(){return"base_chat_model"}getNumTokens(e){if(this._tokenizer===void 0){const r=app._default.default;this._tokenizer=new r({type:"gpt3"})}return this._tokenizer.encode(e).bpe.length}async generatePrompt(e,r){const t=e.map(n=>n.toChatMessages());return this.generate(t,r)}async call(e,r){return(await this.generate([e],r)).generations[0][0].message}async callPrompt(e,r){const t=e.toChatMessages();return this.call(t,r)}}function messageTypeToOpenAIRole(s){switch(s){case"system":return"system";case"ai":return"assistant";case"human":return"user";default:throw new Error(`Unknown message type: ${s}`)}}function openAIResponseToChatMessage(s,e){switch(s){case"user":return new HumanChatMessage(e);case"assistant":return new AIChatMessage(e);case"system":return new SystemChatMessage(e);default:return new ChatMessage(e,s??"unknown")}}class ChatOpenAI extends BaseChatModel{constructor(e,r){if(super(e??{}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"topP",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"frequencyPenalty",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"presencePenalty",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"n",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"logitBias",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"modelName",{enumerable:!0,configurable:!0,writable:!0,value:"gpt-3.5-turbo"}),Object.defineProperty(this,"modelKwargs",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"stop",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streaming",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"maxTokens",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"batchClient",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streamingClient",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"clientConfig",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),!(e?.openAIApiKey??process.env.OPENAI_API_KEY))throw new Error("OpenAI API key not found");if(this.modelName=e?.modelName??this.modelName,this.modelKwargs=e?.modelKwargs??{},this.temperature=e?.temperature??this.temperature,this.topP=e?.topP??this.topP,this.frequencyPenalty=e?.frequencyPenalty??this.frequencyPenalty,this.presencePenalty=e?.presencePenalty??this.presencePenalty,this.maxTokens=e?.maxTokens,this.n=e?.n??this.n,this.logitBias=e?.logitBias,this.stop=e?.stop,this.streaming=e?.streaming??!1,this.streaming&&this.n>1)throw new Error("Cannot stream results when n > 1");this.clientConfig={apiKey:e?.openAIApiKey??process.env.OPENAI_API_KEY,...r}}invocationParams(){return{model:this.modelName,temperature:this.temperature,top_p:this.topP,frequency_penalty:this.frequencyPenalty,presence_penalty:this.presencePenalty,max_tokens:this.maxTokens,n:this.n,logit_bias:this.logitBias,stop:this.stop,stream:this.streaming,...this.modelKwargs}}_identifyingParams(){return{model_name:this.modelName,...this.invocationParams(),...this.clientConfig}}identifyingParams(){return this._identifyingParams()}async _generate(e,r){const t={};if(this.stop&&r)throw new Error("Stop found in input and default params");const n=this.invocationParams();n.stop=r??n.stop;const{data:a}=await this.completionWithRetry({...n,messages:e.map(o=>({role:messageTypeToOpenAIRole(o._getType()),content:o.text}))});if(n.stream){let o="assistant";const p=await new Promise((h,d)=>{let f="";const y=app.createParser(u=>{if(u.type==="event")if(u.data==="[DONE]")h(f);else{const m=JSON.parse(u.data).choices[0];m!=null&&(f+=m.delta?.content??"",o=m.delta?.role??o,this.callbackManager.handleLLMNewToken(m.delta?.content??"",!0))}}),b=a;b.on("data",u=>y.feed(u.toString("utf-8"))),b.on("error",u=>d(u))});return{generations:[{text:p,message:openAIResponseToChatMessage(o,p)}]}}const{completion_tokens:c,prompt_tokens:i,total_tokens:l}=a.usage??{};c&&(t.completionTokens=(t.completionTokens??0)+c),i&&(t.promptTokens=(t.promptTokens??0)+i),l&&(t.totalTokens=(t.totalTokens??0)+l);const g=[];for(const o of a.choices){const p=o.message?.role??void 0,h=o.message?.content??"";g.push({text:h,message:openAIResponseToChatMessage(p,h)})}return{generations:g,llmOutput:{tokenUsage:t}}}async completionWithRetry(e){if(!e.stream&&!this.batchClient){const t=new app.dist.Configuration({...this.clientConfig,baseOptions:{adapter:app.fetchAdapter}});this.batchClient=new app.dist.OpenAIApi(t)}if(e.stream&&!this.streamingClient){const t=new app.dist.Configuration(this.clientConfig);this.streamingClient=new app.dist.OpenAIApi(t)}const r=e.stream?this.streamingClient:this.batchClient;return this.caller.call(r.createChatCompletion.bind(r),e,e.stream?{responseType:"stream"}:void 0)}_llmType(){return"openai"}_combineLLMOutput(...e){return e.reduce((r,t)=>(t&&t.tokenUsage&&(r.tokenUsage.completionTokens+=t.tokenUsage.completionTokens??0,r.tokenUsage.promptTokens+=t.tokenUsage.promptTokens??0,r.tokenUsage.totalTokens+=t.tokenUsage.totalTokens??0),r),{tokenUsage:{completionTokens:0,promptTokens:0,totalTokens:0}})}}exports.ChatOpenAI=ChatOpenAI;
