import{B as O,_ as M,c as A,d as g,f as j}from"../shared/langchainjs.3b9f6c67.mjs";import"path";import"tty";import"util";import"fs";import"net";import"events";import"stream";import"zlib";import"buffer";import"string_decoder";import"querystring";import"url";import"http";import"crypto";import"os";import"https";import"assert";class f{constructor(e){Object.defineProperty(this,"text",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.text=e}}class w extends f{_getType(){return"human"}}class k extends f{_getType(){return"ai"}}class P extends f{_getType(){return"system"}}class T extends f{constructor(e,n){super(e),Object.defineProperty(this,"role",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.role=n}_getType(){return"generic"}}function x(s,e="Human",n="AI"){const t=[];for(const r of s){let a;if(r instanceof w)a=e;else if(r instanceof k)a=n;else if(r instanceof P)a="System";else if(r instanceof T)a=r.role;else throw new Error(`Got unsupported message type: ${r}`);t.push(`${a}: ${r.text}`)}return t.join(`
`)}class I extends O{constructor({...e}){super(e),Object.defineProperty(this,"_tokenizer",{enumerable:!0,configurable:!0,writable:!0,value:void 0})}async generate(e,n){const t=[],r=[],a=e.map(o=>x(o));await this.callbackManager.handleLLMStart({name:this._llmType()},a,this.verbose);try{for(const o of e){const l=await this._generate(o,n);l.llmOutput&&r.push(l.llmOutput),t.push(l.generations)}}catch(o){throw await this.callbackManager.handleLLMError(o,this.verbose),o}const c={generations:t,llmOutput:r.length?this._combineLLMOutput?.(...r):void 0};return await this.callbackManager.handleLLMEnd(c,this.verbose),c}_modelType(){return"base_chat_model"}getNumTokens(e){if(this._tokenizer===void 0){const n=M.default;this._tokenizer=new n({type:"gpt3"})}return this._tokenizer.encode(e).bpe.length}async generatePrompt(e,n){const t=e.map(r=>r.toChatMessages());return this.generate(t,n)}async call(e,n){return(await this.generate([e],n)).generations[0][0].message}async callPrompt(e,n){const t=e.toChatMessages();return this.call(t,n)}}function L(s){switch(s){case"system":return"system";case"ai":return"assistant";case"human":return"user";default:throw new Error(`Unknown message type: ${s}`)}}function _(s,e){switch(s){case"user":return new w(e);case"assistant":return new k(e);case"system":return new P(e);default:return new T(e,s??"unknown")}}class E extends I{constructor(e,n){if(super(e??{}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"topP",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"frequencyPenalty",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"presencePenalty",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"n",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"logitBias",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"modelName",{enumerable:!0,configurable:!0,writable:!0,value:"gpt-3.5-turbo"}),Object.defineProperty(this,"modelKwargs",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"stop",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streaming",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"maxTokens",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"batchClient",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streamingClient",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"clientConfig",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),!(e?.openAIApiKey??process.env.OPENAI_API_KEY))throw new Error("OpenAI API key not found");if(this.modelName=e?.modelName??this.modelName,this.modelKwargs=e?.modelKwargs??{},this.temperature=e?.temperature??this.temperature,this.topP=e?.topP??this.topP,this.frequencyPenalty=e?.frequencyPenalty??this.frequencyPenalty,this.presencePenalty=e?.presencePenalty??this.presencePenalty,this.maxTokens=e?.maxTokens,this.n=e?.n??this.n,this.logitBias=e?.logitBias,this.stop=e?.stop,this.streaming=e?.streaming??!1,this.streaming&&this.n>1)throw new Error("Cannot stream results when n > 1");this.clientConfig={apiKey:e?.openAIApiKey??process.env.OPENAI_API_KEY,...n}}invocationParams(){return{model:this.modelName,temperature:this.temperature,top_p:this.topP,frequency_penalty:this.frequencyPenalty,presence_penalty:this.presencePenalty,max_tokens:this.maxTokens,n:this.n,logit_bias:this.logitBias,stop:this.stop,stream:this.streaming,...this.modelKwargs}}_identifyingParams(){return{model_name:this.modelName,...this.invocationParams(),...this.clientConfig}}identifyingParams(){return this._identifyingParams()}async _generate(e,n){const t={};if(this.stop&&n)throw new Error("Stop found in input and default params");const r=this.invocationParams();r.stop=n??r.stop;const{data:a}=await this.completionWithRetry({...r,messages:e.map(i=>({role:L(i._getType()),content:i.text}))});if(r.stream){let i="assistant";const p=await new Promise((m,v)=>{let d="";const C=A(u=>{if(u.type==="event")if(u.data==="[DONE]")m(d);else{const h=JSON.parse(u.data).choices[0];h!=null&&(d+=h.delta?.content??"",i=h.delta?.role??i,this.callbackManager.handleLLMNewToken(h.delta?.content??"",!0))}}),y=a;y.on("data",u=>C.feed(u.toString("utf-8"))),y.on("error",u=>v(u))});return{generations:[{text:p,message:_(i,p)}]}}const{completion_tokens:c,prompt_tokens:o,total_tokens:l}=a.usage??{};c&&(t.completionTokens=(t.completionTokens??0)+c),o&&(t.promptTokens=(t.promptTokens??0)+o),l&&(t.totalTokens=(t.totalTokens??0)+l);const b=[];for(const i of a.choices){const p=i.message?.role??void 0,m=i.message?.content??"";b.push({text:m,message:_(p,m)})}return{generations:b,llmOutput:{tokenUsage:t}}}async completionWithRetry(e){if(!e.stream&&!this.batchClient){const t=new g.Configuration({...this.clientConfig,baseOptions:{adapter:j}});this.batchClient=new g.OpenAIApi(t)}if(e.stream&&!this.streamingClient){const t=new g.Configuration(this.clientConfig);this.streamingClient=new g.OpenAIApi(t)}const n=e.stream?this.streamingClient:this.batchClient;return this.caller.call(n.createChatCompletion.bind(n),e,e.stream?{responseType:"stream"}:void 0)}_llmType(){return"openai"}_combineLLMOutput(...e){return e.reduce((n,t)=>(t&&t.tokenUsage&&(n.tokenUsage.completionTokens+=t.tokenUsage.completionTokens??0,n.tokenUsage.promptTokens+=t.tokenUsage.promptTokens??0,n.tokenUsage.totalTokens+=t.tokenUsage.totalTokens??0),n),{tokenUsage:{completionTokens:0,promptTokens:0,totalTokens:0}})}}export{E as ChatOpenAI};
