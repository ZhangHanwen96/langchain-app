import{d as k,g as u,h as T,i as C,S as v,A as O,H as A}from"../shared/langchainjs.2e28cd59.mjs";import{B as _}from"../shared/langchainjs.2f6dc440.mjs";import"path";import"tty";import"util";import"fs";import"net";import"events";import"stream";import"zlib";import"buffer";import"string_decoder";import"querystring";import"url";import"http";import"crypto";import"os";import"https";import"assert";import"node:fs/promises";import"node:path";import"child_process";function j(a){switch(a){case"system":return"system";case"ai":return"assistant";case"human":return"user";default:throw new Error(`Unknown message type: ${a}`)}}function d(a,e){switch(a){case"user":return new A(e);case"assistant":return new O(e);case"system":return new v(e);default:return new C(e,a??"unknown")}}class I extends _{constructor(e,n){if(super(e??{}),Object.defineProperty(this,"temperature",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"topP",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"frequencyPenalty",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"presencePenalty",{enumerable:!0,configurable:!0,writable:!0,value:0}),Object.defineProperty(this,"n",{enumerable:!0,configurable:!0,writable:!0,value:1}),Object.defineProperty(this,"logitBias",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"modelName",{enumerable:!0,configurable:!0,writable:!0,value:"gpt-3.5-turbo"}),Object.defineProperty(this,"modelKwargs",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"stop",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streaming",{enumerable:!0,configurable:!0,writable:!0,value:!1}),Object.defineProperty(this,"maxTokens",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"batchClient",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"streamingClient",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"clientConfig",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),!(e?.openAIApiKey??process.env.OPENAI_API_KEY))throw new Error("OpenAI API key not found");if(this.modelName=e?.modelName??this.modelName,this.modelKwargs=e?.modelKwargs??{},this.temperature=e?.temperature??this.temperature,this.topP=e?.topP??this.topP,this.frequencyPenalty=e?.frequencyPenalty??this.frequencyPenalty,this.presencePenalty=e?.presencePenalty??this.presencePenalty,this.maxTokens=e?.maxTokens,this.n=e?.n??this.n,this.logitBias=e?.logitBias,this.stop=e?.stop,this.streaming=e?.streaming??!1,this.streaming&&this.n>1)throw new Error("Cannot stream results when n > 1");this.clientConfig={apiKey:e?.openAIApiKey??process.env.OPENAI_API_KEY,...n}}invocationParams(){return{model:this.modelName,temperature:this.temperature,top_p:this.topP,frequency_penalty:this.frequencyPenalty,presence_penalty:this.presencePenalty,max_tokens:this.maxTokens,n:this.n,logit_bias:this.logitBias,stop:this.stop,stream:this.streaming,...this.modelKwargs}}_identifyingParams(){return{model_name:this.modelName,...this.invocationParams(),...this.clientConfig}}identifyingParams(){return this._identifyingParams()}async _generate(e,n){const t={};if(this.stop&&n)throw new Error("Stop found in input and default params");const i=this.invocationParams();i.stop=n??i.stop;const{data:m}=await this.completionWithRetry({...i,messages:e.map(r=>({role:j(r._getType()),content:r.text}))});if(i.stream){let r="assistant";const s=await new Promise((l,w)=>{let f="";const P=k(o=>{if(o.type==="event")if(o.data==="[DONE]")l(f);else{const p=JSON.parse(o.data).choices[0];p!=null&&(f+=p.delta?.content??"",r=p.delta?.role??r,this.callbackManager.handleLLMNewToken(p.delta?.content??"",!0))}}),y=m;y.on("data",o=>P.feed(o.toString("utf-8"))),y.on("error",o=>w(o))});return{generations:[{text:s,message:d(r,s)}]}}const{completion_tokens:c,prompt_tokens:h,total_tokens:g}=m.usage??{};c&&(t.completionTokens=(t.completionTokens??0)+c),h&&(t.promptTokens=(t.promptTokens??0)+h),g&&(t.totalTokens=(t.totalTokens??0)+g);const b=[];for(const r of m.choices){const s=r.message?.role??void 0,l=r.message?.content??"";b.push({text:l,message:d(s,l)})}return{generations:b,llmOutput:{tokenUsage:t}}}async completionWithRetry(e){if(!e.stream&&!this.batchClient){const t=new u.Configuration({...this.clientConfig,baseOptions:{adapter:T}});this.batchClient=new u.OpenAIApi(t)}if(e.stream&&!this.streamingClient){const t=new u.Configuration(this.clientConfig);this.streamingClient=new u.OpenAIApi(t)}const n=e.stream?this.streamingClient:this.batchClient;return this.caller.call(n.createChatCompletion.bind(n),e,e.stream?{responseType:"stream"}:void 0)}_llmType(){return"openai"}_combineLLMOutput(...e){return e.reduce((n,t)=>(t&&t.tokenUsage&&(n.tokenUsage.completionTokens+=t.tokenUsage.completionTokens??0,n.tokenUsage.promptTokens+=t.tokenUsage.promptTokens??0,n.tokenUsage.totalTokens+=t.tokenUsage.totalTokens??0),n),{tokenUsage:{completionTokens:0,promptTokens:0,totalTokens:0}})}}export{I as ChatOpenAI};
