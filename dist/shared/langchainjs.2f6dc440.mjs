import{c as l,j as u,_ as c}from"./langchainjs.2e28cd59.mjs";class g extends l{constructor({...e}){super(e),Object.defineProperty(this,"_tokenizer",{enumerable:!0,configurable:!0,writable:!0,value:void 0})}async generate(e,t){const s=[],a=[],i=e.map(n=>u(n));await this.callbackManager.handleLLMStart({name:this._llmType()},i,this.verbose);try{for(const n of e){const r=await this._generate(n,t);r.llmOutput&&a.push(r.llmOutput),s.push(r.generations)}}catch(n){throw await this.callbackManager.handleLLMError(n,this.verbose),n}const o={generations:s,llmOutput:a.length?this._combineLLMOutput?.(...a):void 0};return await this.callbackManager.handleLLMEnd(o,this.verbose),o}_modelType(){return"base_chat_model"}getNumTokens(e){if(this._tokenizer===void 0){const t=c.default;this._tokenizer=new t({type:"gpt3"})}return this._tokenizer.encode(e).bpe.length}async generatePrompt(e,t){const s=e.map(a=>a.toChatMessages());return this.generate(s,t)}async call(e,t){return(await this.generate([e],t)).generations[0][0].message}async callPrompt(e,t){const s=e.toChatMessages();return this.call(s,t)}}export{g as B};
